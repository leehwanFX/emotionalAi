{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "from langchain.llms import Ollama\n",
    "from langchain.tools import tool\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "@tool\n",
    "def capture_and_analyze_image() -> str:\n",
    "    \"\"\"보다\"\"\"\n",
    "    # 카메라 캡처\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ret:\n",
    "        return \"카메라에서 이미지를 캡처하는 데 실패했습니다.\"\n",
    "\n",
    "    # 캡처한 이미지 저장\n",
    "    cv2.imwrite('./captured/captured_image.jpg', frame)\n",
    "    print(\"이미지가 저장되었습니다: captured_image.jpg\")\n",
    "\n",
    "    # OpenCV 이미지를 PIL 이미지로 변환\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # 이미지를 base64로 인코딩\n",
    "    buffered = io.BytesIO()\n",
    "    img_pil.save(buffered, format=\"JPEG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "    # llava 모델 초기화 및 이미지 분석\n",
    "    llava = Ollama(model=\"llava-phi3\")\n",
    "    response = llava(\"Analyze this image and describe what you see.\", images=[img_str])\n",
    "\n",
    "    return response\n",
    "\n",
    "# LangChain LLM 초기화 및 도구 바인딩\n",
    "llm = OllamaFunctions(model=\"Bllossom:q8_0\", format=\"json\")\n",
    "llm_with_tools = llm.bind_tools([capture_and_analyze_image])\n",
    "\n",
    "# 함수 사용 예시\n",
    "# response = llm_with_tools.invoke(\"카메라로 사진을 찍고 분석해주세요.\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'보다'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capture_and_analyze_image.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='도구를 활용하여 뭐가 보이는지 알려줘'),\n",
       " AIMessage(content='이미지를 분석할 수 있는 도구는 제공되지 않습니다. 다른 질문이나 도움이 필요하면 말씀해 주세요.', id='run-3db6be62-d51c-4982-b919-c407a9d19c39-0')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "query = \"도구를 활용하여 뭐가 보이는지 알려줘\"\n",
    "messages = [HumanMessage(query)]\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"seeing image\": capture_and_analyze_image}[tool_call[\"name\"].lower()]\n",
    "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(messages[2].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leehwan/miniconda3/envs/ml/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from transformers import pipeline, AutoConfig\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from ollama_function import get_capture\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "# LangChain이 지원하는 다른 채팅 모델을 사용합니다. 여기서는 Ollama를 사용합니다.\n",
    "llm = OllamaFunctions(model=\"Bllossom:q8_0\")\n",
    "\n",
    "model_path = \"../../../BART/final_model\"\n",
    "model_name = \"gogamza/kobart-base-v2\"\n",
    "config = AutoConfig.from_pretrained(model_name, num_labels=2)\n",
    "nlg_pipeline = pipeline('text2text-generation', model=model_path, tokenizer=model_name, config=config, device='mps')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful AI Assistant. Your name is 'ENFP'. You must answer in Korean.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def generate_text(text):\n",
    "    text = f\"enfp 말투로 변환:{text}\"\n",
    "    out = nlg_pipeline(text, max_length=60)\n",
    "    return out[0]['generated_text']\n",
    "\n",
    "def format_output(text):\n",
    "    return text.strip()\n",
    "\n",
    "@tool\n",
    "def get_img():\n",
    "    \"\"\"눈 앞에 보이는 것을 분석해줍니다.\"\"\"\n",
    "    query = \"눈 앞에 보이는 것을 카메라로 찍어 설명해주세요\"\n",
    "    return get_capture(query=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools=[get_img])\n",
    "query = \"눈 앞에 보이는 것을 카메라로 찍어 설명해주세요\"\n",
    "response = llm_with_tools.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser() | generate_text | format_output | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful AI Vision Assistant.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"explain {english_word} using oxford dictionary to me in Korean.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re(prompt):\n",
    "    prompt1 = prompt.to_messages\n",
    "    return {\"english_word\": prompt1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english_word': <bound method ChatPromptValue.to_messages of ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI Vision Assistant.'), HumanMessage(content='test')])>}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke([\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful AI Vision Assistant.')), MessagesPlaceholder(variable_name='messages')])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
