{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 데이터 확인\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dataset 만들기\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Detect Face\n",
    "import cv2\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu사용 변수\n",
    "device = torch.device('mps:0') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/face_detect/fer2013.csv')\n",
    "\n",
    "# 이미지 픽셀 list로 만들기\n",
    "df['pixels'] = df['pixels'].apply(lambda pixel: np.fromstring(pixel, sep=' '))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test 나누기\n",
    "train_df = df[df['Usage']=='Training']\n",
    "test_df = df[df['Usage']=='PublicTest']\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_x = 48\n",
    "shape_y = 48\n",
    "\n",
    "# X_train, y_train, X_test, y_test split\n",
    "X_train = train_df.iloc[:, 1].values # pixles\n",
    "y_train = train_df.iloc[:, 0].values # emotion\n",
    "\n",
    "X_test = test_df.iloc[:, 1].values # pixles\n",
    "y_test = test_df.iloc[:, 0].values # emotion\n",
    "\n",
    "# 전체데이터\n",
    "X = df.iloc[:, 1].values # pixles\n",
    "y = df.iloc[:, 0].values # emotion\n",
    "\n",
    "# array([array([....])]) 구조를 바꾸기 위한 np.vstack\n",
    "X_train = np.vstack(X_train)\n",
    "X_test = np.vstack(X_test)\n",
    "X = np.vstack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4차원 데이터셋 만들기 (데이터개수, x축, y축, rgb)\n",
    "X_train_ds = np.reshape(X_train, (X_train.shape[0], shape_x, shape_y, 1))\n",
    "y_train_ds = np.reshape(y_train, (y_train.shape[0], 1))\n",
    "\n",
    "X_test_ds = np.reshape(X_test, (X_test.shape[0], shape_x, shape_y, 1))\n",
    "y_test_ds = np.reshape(y_test, (y_test.shape[0], 1))\n",
    "\n",
    "print(X_train_ds.shape, y_train_ds.shape)\n",
    "print(X_test_ds.shape, y_test_ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터타입 float로 변경\n",
    "train_data = X_train_ds.astype('float32')\n",
    "test_data = X_test_ds.astype('float32')\n",
    "\n",
    "# 스케일링\n",
    "train_data /= 225\n",
    "test_data /= 225\n",
    "\n",
    "# y데이터 원핫인코딩\n",
    "train_labels_onehot = to_categorical(y_train_ds)\n",
    "test_labels_onehot = to_categorical(y_test_ds)\n",
    "\n",
    "# input_shape 설정\n",
    "n_rows, n_cols, n_dims = X_train_ds.shape[1:]\n",
    "input_shape = (n_rows, n_cols, n_dims)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 숫자를 문자로 변경\n",
    "def get_label(argument):\n",
    "    labels = {0:'angry', 1:'disgust', 2:'fear', 3:'happy', 4:'sad', 5:'surprise', 6:'neutral'}\n",
    "    return(labels.get(argument, 'Invalid emotion'))\n",
    "    \n",
    "# 데이터 시각화\n",
    "plt.figure(figsize=[10,5])\n",
    "\n",
    "# Train data 중 100번째 이미지\n",
    "n=100\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.squeeze(X_train_ds[n,:,:], axis = 2), cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(get_label(int(y_train[n]))))\n",
    "\n",
    "# Test data 중 100번째 이미지\n",
    "plt.subplot(122)\n",
    "plt.imshow(np.squeeze(X_test_ds[n,:,:], axis = 2), cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(get_label(int(y_test[n]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 이미지에서 얼굴을 찾아내는 함수\n",
    "def detect_face(frame):\n",
    "    # cascade pre-trained 모델 불러오기\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    # RGB를 gray scale로 바꾸기\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # cascade 멀티스케일 분류\n",
    "    detected_faces = face_cascade.detectMultiScale(gray,\n",
    "                                                   scaleFactor=1.1,\n",
    "                                                   minNeighbors=6,\n",
    "                                                   minSize=(shape_x, shape_y),\n",
    "                                                   flags=cv2.CASCADE_SCALE_IMAGE\n",
    "                                                   )\n",
    "    coord = []\n",
    "    for x, y, w, h in detected_faces:\n",
    "        if w > 100:\n",
    "            sub_img = frame[y:y+h, x:x+w]\n",
    "            coord.append([x, y, w, h])\n",
    "    return gray, detected_faces, coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 이미지에서 찾아낸 얼굴을 추출하는 함수\n",
    "def extract_face_features(gray, detected_faces, coord, offset_coefficients=(0.075, 0.05)):\n",
    "    new_face = []\n",
    "    for det in detected_faces:\n",
    "        # 얼굴로 감지된 영역\n",
    "        x, y, w, h = det\n",
    "        # 이미지 경계값 받기\n",
    "        horizontal_offset = int(np.floor(offset_coefficients[0] * w))  # 여기를 수정\n",
    "        vertical_offset = int(np.floor(offset_coefficients[1] * h))    # 여기를 수정\n",
    "        # gray scacle 에서 해당 위치 가져오기\n",
    "        extracted_face = gray[y+vertical_offset:y+h, x+horizontal_offset:x-horizontal_offset+w]\n",
    "        # 얼굴 이미지만 확대\n",
    "        new_extracted_face = zoom(extracted_face, (shape_x/extracted_face.shape[0], shape_y/extracted_face.shape[1]))\n",
    "        new_extracted_face = new_extracted_face.astype(np.float32)\n",
    "        new_extracted_face /= float(new_extracted_face.max())  # scaled\n",
    "        new_face.append(new_extracted_face)\n",
    "    return new_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suzy = cv2.imread('./data/face_detect/e_neutral.png')\n",
    "plt.imshow(cv2.cvtColor(suzy, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 얼굴 찾기\n",
    "gray, detected_faces, coord = detect_face(suzy)\n",
    "\n",
    "# 찾은 얼굴 추출하기\n",
    "face_zoom = extract_face_features(gray, detected_faces, coord)\n",
    "\n",
    "# 시각화\n",
    "plt.imshow(face_zoom[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    \n",
    "    # Add layers\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Flatten\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Fully connected layer\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    \n",
    "    # Output layer : n_classes=7\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 레이어마다 어떻게 변화하는지 시각화\n",
    "\n",
    "# 마지막 Dense 레이어 2개를 제외한 레이어별 out 담기 \n",
    "layer_outputs = [layer.output for layer in model.layers[:12]]\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "# 시각화 할 때 어떤 레이어인지 확인할 수 있도록 레이어 이름 담기\n",
    "layer_names = []\n",
    "for layer in model.layers[:12]:\n",
    "    layer_names.append(layer.name)\n",
    "    \n",
    "# 수지 얼굴 이미지 가져오기\n",
    "suzy = cv2.imread('./data/face_detect/e_neutral.png')\n",
    "gray, detected_faces, coord = detect_face(suzy)\n",
    "face_zoom = extract_face_features(gray, detected_faces, coord)\n",
    "face = face_zoom[0]\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title(\"Original Face\")\n",
    "plt.imshow(suzy)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Extracted Face\")\n",
    "plt.imshow(face)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수지 얼굴 모델에 넣어서 모델이 어떻게 작동하는지 시각화\n",
    "to_predict = np.reshape(face.flatten(), (1,48,48,1))\n",
    "res = model.predict(to_predict)\n",
    "activations = activation_model.predict(to_predict)\n",
    "\n",
    "images_per_row = 16\n",
    "\n",
    "for layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n",
    "    n_features = layer_activation.shape[-1] # Number of features in the feature map\n",
    "    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n",
    "    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,:, :,col * images_per_row + row]\n",
    "            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size, # Displays the grid\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터 증강\n",
    "datagen = ImageDataGenerator(zoom_range=0.2,          # 랜덤하게 이미지 줌 하는 비율\n",
    "                             rotation_range=10,       # 램덤하게 이미지 회전하는 비율 (0도~180도)\n",
    "                             width_shift_range=0.1,   # 랜덤하게 이미지 가로로 이동하는 비율\n",
    "                             height_shift_range=0.1,  # 랜덤하게 이미지 세로로 이동하는 비율\n",
    "                             horizontal_flip=True,    # 랜덤하게 이미지 수평 뒤집기\n",
    "                             vertical_flip=False)     # 랜덤하게 이미지 수직 뒤집기\n",
    "                             \n",
    "# 모델 학습을 위한 파라미터 설정\n",
    "batch_size = 256\n",
    "n_epochs = 100\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit_generator(datagen.flow(train_data, train_labels_onehot, batch_size=batch_size),\n",
    "                              steps_per_epoch=int(np.ceil(train_data.shape[0]/float(batch_size))),\n",
    "                              epochs=n_epochs,\n",
    "                              validation_data=(test_data, test_labels_onehot)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SavedModel 형식으로 저장\n",
    "import tensorflow as tf\n",
    "tf.saved_model.save(model, './face-emotion-model')\n",
    "\n",
    "# 나중에 SavedModel 불러오기\n",
    "loaded_model = tf.saved_model.load('./face-emotion-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 모델 저장\n",
    "model.save('./face-emotion-model/model.h5')\n",
    "\n",
    "# 나중에 모델 불러오기\n",
    "# from tensorflow.keras.models import load_model\n",
    "# loaded_model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['loss'],'r',linewidth=2.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=2.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    " \n",
    "# Accuracy Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['accuracy'],'r',linewidth=2.0)\n",
    "plt.plot(history.history['val_accuracy'],'b',linewidth=2.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본이미지 확인\n",
    "face = cv2.imread('./data/face_detect/1.png')\n",
    "\n",
    "# 얼굴 추출\n",
    "gray, detected_faces, coord = detect_face(face)\n",
    "face_zoom = extract_face_features(gray, detected_faces, coord)\n",
    "\n",
    "# 모델 추론\n",
    "input_data = np.reshape(face_zoom[0].flatten(), (1, 48, 48, 1))\n",
    "output_data = model.predict(input_data)\n",
    "result = np.argmax(output_data)\n",
    "\n",
    "# 결과 문자로 변환\n",
    "if result == 0:\n",
    "    emotion = 'angry'\n",
    "elif result == 1:\n",
    "    emotion = 'disgust'\n",
    "elif result == 2:\n",
    "    emotion = 'fear'\n",
    "elif result == 3:\n",
    "    emotion = 'happy'\n",
    "elif result == 4:\n",
    "    emotion = 'sad'\n",
    "elif result == 5:\n",
    "    emotion = 'surprise'\n",
    "elif result == 6:\n",
    "    emotion = 'neutral'\n",
    "    \n",
    "# 시각화\n",
    "plt.subplot(121)\n",
    "plt.title(\"Original Face\")\n",
    "plt.imshow(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(f\"Extracted Face : {emotion}\")\n",
    "plt.imshow(face_zoom[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감정 레이블 매핑\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "# 이미지 처리 및 시각화 함수\n",
    "def process_and_visualize(image_path):\n",
    "    # 원본 이미지 읽기\n",
    "    face = cv2.imread(image_path)\n",
    "    \n",
    "    # 얼굴 추출\n",
    "    gray, detected_faces, coord = detect_face(face)\n",
    "    face_zoom = extract_face_features(gray, detected_faces, coord)\n",
    "    \n",
    "    if len(face_zoom) == 0:\n",
    "        print(f\"No face detected in {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # 모델 추론\n",
    "    input_data = np.reshape(face_zoom[0].flatten(), (1, 48, 48, 1))\n",
    "    output_data = model.predict(input_data)\n",
    "    result = np.argmax(output_data)\n",
    "    \n",
    "    # 결과 문자로 변환\n",
    "    emotion = emotion_labels[result]\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.title(\"Original Face\")\n",
    "    plt.imshow(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.title(f\"Extracted Face : {emotion}\")\n",
    "    plt.imshow(face_zoom[0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 1.png부터 7.png까지 반복 처리\n",
    "for i in range(1, 7):\n",
    "    image_path = f'./data/face_detect/{i}.png'\n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    process_and_visualize(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
